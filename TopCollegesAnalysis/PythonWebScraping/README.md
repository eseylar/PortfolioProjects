## Web-Scraping Using Python and Beautiful Soup

**The first element of this exploratory data analysis was to actually extract the data needed for analysis by using the Python web-scraping library, Beautiful Soup.**

After looking at several college ranking websites online, I found that Niche.com had the most information readily available for web-scraping. (Other websites hid quantitative information like acceptance rate or SAT scores behind a paywall; Niche left that information readily accessible.) 

I used the Beautiful Soup library inside of Google Colaboratory, a Jupyter Notebook environment, to extract information about top colleges. Niche's ranking of top colleges in 2023 goes on for over 100 pages. With 25 entries per page, Niche effective ranks every school in its database. While that is a nice, robust set of data to have access to, looking at the school ranked #2543 in the nation does not help much with an analysis of *top* colleges. Thus, I limited my search to the first 250 top schools, or 10 pages of results. 

Using Beautiful Soup and a page-counting loop, I generated a script that extracts the following information about the top 250 schools on Niche's list: `name`, `rank`, `city_state` (location as "City, ST"), `city`, `state`, `acceptance` (acceptance rate as integer), `price` (average price after aid), `SAT_range` (the range of admitted students' SAT scores as a string, SAT low - SAT high), `SAT_low` (the low value of the range, as a numeric value), and `SAT_high` (the high value of the SAT range, as a numeric value). The above data were put into a dataframe using the Pandas library. The data frame was extracted to a .csv file for analysis in SQL and in another Jupyter Notebook. The code for this script can be found in this repository as ["Web_Scraping_Niche_2023_Best_Colleges_in_America.ipynb"](https://github.com/eseylar/PortfolioProjects/blob/main/TopCollegesAnalysis/PythonWebScraping/Web_Scraping_Niche_2023_Best_Colleges_in_America.ipynb).

One problem that arose while crafting this data-scraping script was a limitation of the Google Colab environment. Niche's website requires JavaScript to be enabled to access site data; the HTML data cannot be scraped without JS enabled. One common workaround, and a workaround I attempted, is to enable the Selenium library in Python. This library allows users to write code that effectively tricks a website into thinking an automated script is a user with JS enabled.

After several attempts to install and utilize Selenium, I discovered that a recent update to Google Colab makes it exceedingly difficult to run Selenium. One option for me would be to move my workflow to another Jupyter Notebook that allows Selenium. Were I extracting data from a large number of webpages, I likely would have gone down this route. However, my data exploration only required me to pull data from 10 different pages. Because my need was so little, I chose, instead, to manually go to each webpage and save as HTML. I then uploaded these HTML files directly to my Notebook and altered my code to read the files instead. Code for both parsing from URL and from uploaded file are in my web-scraping Notebook file. The uploaded HTML files can be accessed [here](https://github.com/eseylar/PortfolioProjects/tree/main/TopCollegesAnalysis/PythonWebScraping/NicheSiteHTML).

The file [top_250_colleges.csv](https://github.com/eseylar/PortfolioProjects/blob/main/TopCollegesAnalysis/PythonWebScraping/top_250_colleges.csv) contains the results from my web-scraping script.
